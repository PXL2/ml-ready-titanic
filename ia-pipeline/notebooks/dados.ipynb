{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Carregar os dados\n",
    "csv_path = \"../data/raw/Titanic-Dataset.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "# 1. Tratamento de valores ausentes\n",
    "df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "df['Title'] = df['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "df['Title'] = df['Title'].replace(['Mme'], 'Mrs')\n",
    "df['Title'] = df['Title'].replace(['Lady', 'Countess', 'Dona'], 'Royalty')\n",
    "df['Title'] = df['Title'].replace(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer')\n",
    "df['Title'] = df['Title'].replace(['Don', 'Sir', 'Jonkheer'], 'Sir')\n",
    "\n",
    "# Criar feature de tamanho da família\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "\n",
    "# Preencher valores ausentes\n",
    "df['Age'] = df.groupby(['Pclass', 'Sex', 'Title'])['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "df['Embarked'] = df.groupby('Pclass')['Embarked'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "df['Fare'] = df.groupby(['Pclass', 'FamilySize'])['Fare'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Feature Engineering\n",
    "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "df['HasCabin'] = (~df['Cabin'].isna()).astype(int)\n",
    "df['FarePerPerson'] = df['Fare'] / df['FamilySize']\n",
    "\n",
    "# Selecionar features para o modelo\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', \n",
    "           'Title', 'FamilySize', 'IsAlone', 'HasCabin', 'FarePerPerson']\n",
    "\n",
    "# Codificar variáveis categóricas\n",
    "le = LabelEncoder()\n",
    "categorical_features = ['Sex', 'Embarked', 'Title']\n",
    "for feature in categorical_features:\n",
    "    df[feature] = le.fit_transform(df[feature])\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X = df[features]\n",
    "y = df['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Aplicar SMOTE para balancear as classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Definir os modelos base\n",
    "rf = RandomForestClassifier(random_state=42, class_weight={0: 1, 1: 3})\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000, class_weight={0: 1, 1: 3})\n",
    "svm = SVC(random_state=42, probability=True, class_weight={0: 1, 1: 3})\n",
    "\n",
    "# Definir os parâmetros para GridSearch\n",
    "param_grid = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [300, 400, 500],\n",
    "        'max_depth': [20, 25, 30],\n",
    "        'min_samples_split': [2, 3],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [300, 400, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [4, 5, 6],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dicionário para armazenar os resultados\n",
    "results = {}\n",
    "best_models = {}\n",
    "\n",
    "# Definir StratifiedKFold para validação cruzada\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Treinar e avaliar cada modelo com GridSearch\n",
    "for name, model in [('Random Forest', rf), ('Gradient Boosting', gb),\n",
    "                   ('Logistic Regression', lr), ('SVM', svm)]:\n",
    "    print(f\"\\nTreinando {name}...\")\n",
    "    \n",
    "    # Realizar GridSearch\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid[name], \n",
    "        cv=skf, \n",
    "        scoring='precision',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Obter o melhor modelo\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[name] = best_model\n",
    "    \n",
    "    # Fazer previsões\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Calcular acurácia\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Realizar validação cruzada\n",
    "    cv_scores = cross_val_score(best_model, X, y, cv=skf)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'best_params': grid_search.best_params_\n",
    "    }\n",
    "    \n",
    "    # Imprimir relatório de classificação\n",
    "    print(f\"\\nRelatório de Classificação para {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Imprimir resultados da validação cruzada\n",
    "    print(f\"\\nResultados da Validação Cruzada para {name}:\")\n",
    "    print(f\"Média: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    print(f\"Melhores parâmetros: {grid_search.best_params_}\")\n",
    "\n",
    "# Criar um ensemble com Stacking\n",
    "estimators = [(name, model) for name, model in best_models.items()]\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=skf\n",
    ")\n",
    "\n",
    "# Treinar o ensemble\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar o ensemble\n",
    "y_pred_ensemble = stacking_clf.predict(X_test)\n",
    "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
    "cv_scores_ensemble = cross_val_score(stacking_clf, X, y, cv=skf)\n",
    "\n",
    "results['Stacking'] = {\n",
    "    'accuracy': accuracy_ensemble,\n",
    "    'cv_mean': cv_scores_ensemble.mean(),\n",
    "    'cv_std': cv_scores_ensemble.std()\n",
    "}\n",
    "\n",
    "print(\"\\nResultados do Stacking Ensemble:\")\n",
    "print(f\"Acurácia: {accuracy_ensemble:.3f}\")\n",
    "print(f\"Validação Cruzada: {cv_scores_ensemble.mean():.3f} (+/- {cv_scores_ensemble.std() * 2:.3f})\")\n",
    "\n",
    "# Encontrar o melhor modelo\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['cv_mean'])[0]\n",
    "print(f\"\\nMelhor modelo: {best_model_name}\")\n",
    "print(f\"Acurácia: {results[best_model_name]['accuracy']:.3f}\")\n",
    "print(f\"Validação Cruzada: {results[best_model_name]['cv_mean']:.3f} (+/- {results[best_model_name]['cv_std'] * 2:.3f})\")\n",
    "\n",
    "# Salvar o melhor modelo\n",
    "import joblib\n",
    "if best_model_name == 'Stacking':\n",
    "    best_model_instance = stacking_clf\n",
    "else:\n",
    "    best_model_instance = best_models[best_model_name]\n",
    "\n",
    "model_path = os.path.join(os.getcwd(), 'best_model.joblib')\n",
    "joblib.dump(best_model_instance, model_path)\n",
    "print(f\"\\nMelhor modelo salvo em: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Avaliar o ensemble\n",
    "y_pred_ensemble = stacking_clf.predict(X_test)\n",
    "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
    "cv_scores_ensemble = cross_val_score(stacking_clf, X, y, cv=skf)\n",
    "\n",
    "results['Stacking'] = {\n",
    "    'accuracy': accuracy_ensemble,\n",
    "    'cv_mean': cv_scores_ensemble.mean(),\n",
    "    'cv_std': cv_scores_ensemble.std()\n",
    "}\n",
    "\n",
    "# Criar e mostrar matriz de confusão para o ensemble\n",
    "cm_ensemble = confusion_matrix(y_test, y_pred_ensemble)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm_ensemble, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Não Sobreviveu', 'Sobreviveu'],\n",
    "            yticklabels=['Não Sobreviveu', 'Sobreviveu'])\n",
    "plt.title('Matriz de Confusão - Stacking Ensemble')\n",
    "plt.ylabel('Verdadeiro')\n",
    "plt.xlabel('Predito')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResultados do Stacking Ensemble:\")\n",
    "print(f\"Acurácia: {accuracy_ensemble:.3f}\")\n",
    "print(f\"Validação Cruzada: {cv_scores_ensemble.mean():.3f} (+/- {cv_scores_ensemble.std() * 2:.3f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
