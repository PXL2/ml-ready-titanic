# Pipeline de IA para PrediÃ§Ã£o de SobrevivÃªncia no Titanic

Este projeto implementa um pipeline completo de Machine Learning, desde o prÃ©-processamento e limpeza de dados do famoso dataset do Titanic atÃ© o treinamento, otimizaÃ§Ã£o e avaliaÃ§Ã£o de modelos preditivos. O objetivo principal Ã© classificar os passageiros entre sobreviventes e nÃ£o sobreviventes, com foco em alcanÃ§ar alta precisÃ£o para ambas as classes.

O trabalho original de prÃ©-processamento foi desenvolvido em colaboraÃ§Ã£o por Pedro Lima e Carla Santana, e este repositÃ³rio expande essa base para um pipeline de IA de ponta a ponta.

## ğŸš€ Funcionalidades Principais

O pipeline Ã© composto pelas seguintes etapas:

### 1. PrÃ©-processamento e Limpeza de Dados (`ia-pipeline/data/processed/processados.py`)
* **Carregamento de Dados:** Leitura do dataset bruto.
* **Engenharia de Features Inicial:**
    * ExtraÃ§Ã£o de `Title` (tÃ­tulo) a partir dos nomes dos passageiros e normalizaÃ§Ã£o de tÃ­tulos raros.
* **Tratamento de Valores Ausentes:**
    * `Age`: ImputaÃ§Ã£o pela mediana agrupada por `Pclass`, `Sex` e `Title`.
    * `Embarked`: ImputaÃ§Ã£o pela moda agrupada por `Pclass`.
    * `Fare`: ImputaÃ§Ã£o pela mediana agrupada por `Pclass` e `FamilySize`.
    * `Cabin`: Transformada na feature binÃ¡ria `HasCabin`.
* **Engenharia de Features Adicional:**
    * CriaÃ§Ã£o de `FamilySize` (SibSp + Parch + 1).
    * CriaÃ§Ã£o de `IsAlone` (baseado no `FamilySize`).
    * CriaÃ§Ã£o de `FarePerPerson` (Fare / FamilySize).
* **CodificaÃ§Ã£o de VariÃ¡veis CategÃ³ricas:**
    * TransformaÃ§Ã£o das features `Sex`, `Embarked` e `Title` em representaÃ§Ãµes numÃ©ricas usando `LabelEncoder`.
* **SeleÃ§Ã£o e RemoÃ§Ã£o de Colunas:** RemoÃ§Ã£o de colunas nÃ£o informativas ou redundantes (`PassengerId`, `Name`, `Ticket`, `Cabin` (original), `SibSp`, `Parch`).
* **ExportaÃ§Ã£o:** Salvamento do dataset prÃ©-processado para a prÃ³xima etapa.

### 2. Treinamento e OtimizaÃ§Ã£o de Modelos (`ia-pipeline/data/processed/treino/train.py`)
* **Carregamento de Dados Processados.**
* **DivisÃ£o dos Dados:** SeparaÃ§Ã£o em conjuntos de treino e teste, com estratificaÃ§Ã£o pela variÃ¡vel alvo (`Survived`).
* **Balanceamento de Classes (SMOTE):**
    * AplicaÃ§Ã£o da tÃ©cnica SMOTE (Synthetic Minority Over-sampling Technique) *apenas* no conjunto de treinamento para lidar com o desbalanceamento entre sobreviventes e nÃ£o sobreviventes.

    **Antes do SMOTE:**
    A distribuiÃ§Ã£o original da variÃ¡vel alvo no conjunto de treino mostra um desbalanceamento.

    ![DistribuiÃ§Ã£o da VariÃ¡vel Alvo (y_train) - ANTES do SMOTE](ia-pipeline/assets/imagens/Antes%20do%20SMOTE.png)

    *Figura 1: DistribuiÃ§Ã£o da variÃ¡vel alvo no conjunto de treino antes da aplicaÃ§Ã£o do SMOTE.*


    **Depois do SMOTE:**
    ApÃ³s a aplicaÃ§Ã£o do SMOTE, as classes no conjunto de treino ficam balanceadas, auxiliando o modelo a aprender de forma mais eficaz.

    ![DistribuiÃ§Ã£o da VariÃ¡vel Alvo (y_train_smote) - DEPOIS do SMOTE](ia-pipeline/assets/imagens/Depois%20do%20SMOTE.png)

    *Figura 2: DistribuiÃ§Ã£o da variÃ¡vel alvo no conjunto de treino apÃ³s a aplicaÃ§Ã£o do SMOTE.*

* **Pipeline de Modelagem:**
    * Uso de `StandardScaler` para normalizar os dados antes do treinamento.
    * Treinamento de mÃºltiplos algoritmos de classificaÃ§Ã£o: RegressÃ£o LogÃ­stica, SVM, Random Forest, Gradient Boosting e um ensemble Stacking.
* **OtimizaÃ§Ã£o de HiperparÃ¢metros:**
    * UtilizaÃ§Ã£o de `GridSearchCV` com validaÃ§Ã£o cruzada estratificada (`StratifiedKFold`) para encontrar a melhor combinaÃ§Ã£o de hiperparÃ¢metros para cada modelo.
    * MÃ©trica de otimizaÃ§Ã£o: `precision_macro` para buscar um bom equilÃ­brio de precisÃ£o entre as classes.
    * ImplementaÃ§Ã£o de cache para os resultados do `GridSearchCV` para acelerar execuÃ§Ãµes futuras.
* **Ajuste de Limiar de DecisÃ£o (Thresholding):**
    * Desenvolvimento de uma lÃ³gica customizada para encontrar o limiar de probabilidade Ã³timo que maximize a chance de atingir as metas de precisÃ£o (>80%) para ambas as classes (sobreviventes e nÃ£o sobreviventes).
* **AvaliaÃ§Ã£o e SeleÃ§Ã£o do Melhor Modelo:**
    * AnÃ¡lise detalhada da performance dos modelos usando `classification_report` e `confusion_matrix`.
    * SeleÃ§Ã£o do modelo final com base no desempenho e nas metas de precisÃ£o estabelecidas.
* **Salvamento do Modelo:** O melhor modelo treinado e seu limiar Ã³timo sÃ£o salvos em disco (`.joblib`) para futuras prediÃ§Ãµes ou deploy.

## ğŸ› ï¸ Tecnologias Utilizadas
* Python 3.x
* Pandas
* NumPy
* Scikit-learn
* Imbalanced-learn (para SMOTE)
* Matplotlib / Seaborn (para visualizaÃ§Ãµes, embora nÃ£o geradas diretamente pelos scripts `processados.py` ou `train.py`, foram base para as imagens de SMOTE)
* Joblib (para salvar modelos e cache)

## ğŸ“ Estrutura do Projeto

A estrutura de pastas do projeto estÃ¡ organizada da seguinte forma (considerando a raiz como `PYTHON SUDESTE` localmente):

PYTHON SUDESTE/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ ia-pipeline/
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â””â”€â”€ imagens/
â”‚   â”‚       â”œâ”€â”€ Antes do SMOTE.png
â”‚   â”‚       â””â”€â”€ Depois do SMOTE.png
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”‚   â”œâ”€â”€ processados.py
â”‚   â”‚   â”‚   â”œâ”€â”€ processed_titanic_data.csv
â”‚   â”‚   â”‚   â””â”€â”€ treino/
â”‚   â”‚   â”‚       â””â”€â”€ train.py
â”‚   â”‚   â””â”€â”€ raw/
â”‚   â”‚       â””â”€â”€ Titanic-Dataset.csv
â”‚   â”œâ”€â”€ env/                    # Ambiente virtual (sugestÃ£o)
â”‚   â”œâ”€â”€ models/                 # Modelos treinados salvos
â”‚   â”œâ”€â”€ models_cache/           # Cache do GridSearchCV
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â””â”€â”€ dados.ipynb         # Notebook original de exploraÃ§Ã£o e desenvolvimento
â”‚   â””â”€â”€ src/                    # CÃ³digo fonte adicional (ex: API)
â”‚       â””â”€â”€ api/
â”‚           â””â”€â”€ app.py
â””â”€â”€ requirements.txt            # DependÃªncias do projeto (sugestÃ£o de criaÃ§Ã£o)

## ğŸš€ ComeÃ§o RÃ¡pido

Siga os passos abaixo para executar o pipeline:

1.  **Clone o RepositÃ³rio:**
    ```bash
    git clone [URL_DO_SEU_REPOSITORIO_AQUI]
    cd NOME_DA_PASTA_DO_PROJETO_LOCAL # Ex: cd PYTHON SUDESTE
    ```

2.  **Crie e Ative um Ambiente Virtual (Recomendado):**
    ```bash
    python -m venv env
    # No Linux/macOS:
    source env/bin/activate
    # No Windows:
    # env\Scripts\activate
    ```

3.  **Instale as DependÃªncias:**
    (Crie um arquivo `requirements.txt` com as bibliotecas listadas em "Tecnologias Utilizadas" se ainda nÃ£o o fez)
    ```bash
    pip install -r requirements.txt
    ```
    Se nÃ£o tiver um `requirements.txt`, instale manualmente:
    ```bash
    pip install pandas numpy scikit-learn imbalanced-learn joblib matplotlib seaborn
    ```

4.  **Execute o Script de PrÃ©-processamento:**
    Este script irÃ¡ limpar os dados e gerar o `processed_titanic_data.csv`.
    ```bash
    python ia-pipeline/data/processed/processados.py
    ```

5.  **Execute o Script de Treinamento:**
    Este script carregarÃ¡ os dados processados, treinarÃ¡ os modelos e salvarÃ¡ o melhor modelo.
    ```bash
    python ia-pipeline/data/processed/treino/train.py
    ```

## ğŸ¯ Resultados Esperados
O pipeline busca identificar o melhor modelo de classificaÃ§Ã£o que atinja uma **precisÃ£o de pelo menos 80% tanto para prever passageiros que sobreviveram quanto para os que nÃ£o sobreviveram**, apÃ³s ajuste fino do limiar de decisÃ£o. Os resultados detalhados e o modelo selecionado sÃ£o exibidos ao final da execuÃ§Ã£o do script de treinamento.
